{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import re\nimport string\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dense, LSTM, Embedding, Bidirectional","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-08-21T15:26:06.057427Z","iopub.execute_input":"2022-08-21T15:26:06.058072Z","iopub.status.idle":"2022-08-21T15:26:06.087928Z","shell.execute_reply.started":"2022-08-21T15:26:06.058033Z","shell.execute_reply":"2022-08-21T15:26:06.087131Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"code","source":"# Text preprocessing function\n\nstr_punc = string.punctuation.replace(',', '').replace(\"'\",'')\n\ndef clean(text):\n    global str_punc\n    text = re.sub(r'[^a-zA-Z ]', '', text)\n    text = text.lower()\n    return text    ","metadata":{"execution":{"iopub.status.busy":"2022-08-21T15:26:08.415185Z","iopub.execute_input":"2022-08-21T15:26:08.415805Z","iopub.status.idle":"2022-08-21T15:26:08.421433Z","shell.execute_reply.started":"2022-08-21T15:26:08.415769Z","shell.execute_reply":"2022-08-21T15:26:08.419921Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"# Read dataset & Preprocess text\n# dataset = pd.read_csv('../input/isear-emotion/isear_dataset.csv')\ndf_train = pd.read_csv('../input/emotions-dataset-for-nlp/train.txt', names=['Text', 'Emotion'], sep=';')\ndf_val = pd.read_csv('../input/emotions-dataset-for-nlp/val.txt', names=['Text', 'Emotion'], sep=';')\ndf_test = pd.read_csv('../input/emotions-dataset-for-nlp/test.txt', names=['Text', 'Emotion'], sep=';')\n\ndf_train = df_train.loc[(df_train.Emotion!='surprise') & (df_train.Emotion!='love')]\ndf_val = df_val.loc[(df_val.Emotion!='surprise') & (df_val.Emotion!='love')]\ndf_test = df_test.loc[(df_test.Emotion!='surprise') & (df_test.Emotion!='love')]\n\nX_train = df_train['Text'].apply(clean)\ny_train = df_train['Emotion']\n\nX_test = df_test['Text'].apply(clean)\ny_test = df_test['Emotion']\n\nX_val = df_val['Text'].apply(clean)\ny_val = df_val['Emotion']","metadata":{"execution":{"iopub.status.busy":"2022-08-21T15:35:52.345774Z","iopub.execute_input":"2022-08-21T15:35:52.346583Z","iopub.status.idle":"2022-08-21T15:35:52.437152Z","shell.execute_reply.started":"2022-08-21T15:35:52.346542Z","shell.execute_reply":"2022-08-21T15:35:52.436440Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"# Visualize classes counts\n# colors = np.array([\"gray\", 'orangered', 'hotpink', 'gold', 'teal', 'cornflowerblue'])\ncolors = np.array([\"gray\", 'orangered', 'hotpink', 'gold'])\n\nplt.bar(y_train.unique(),height=y_train.value_counts(), color=colors)\nplt.title(\"Training data - classes counts\")\nplt.show()\n\nplt.bar(y_test.unique(),height=y_test.value_counts(), color=colors)\nplt.title(\"Testing data - classes counts\")\nplt.show()\n\nplt.bar(y_val.unique(),height=y_val.value_counts(), color=colors)\nplt.title(\"Validation data - classes counts\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-21T15:36:09.768765Z","iopub.execute_input":"2022-08-21T15:36:09.769035Z","iopub.status.idle":"2022-08-21T15:36:10.480395Z","shell.execute_reply.started":"2022-08-21T15:36:09.769005Z","shell.execute_reply":"2022-08-21T15:36:10.479702Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"# Encode labels\n\nle = LabelEncoder()\ny_train = le.fit_transform(y_train)\ny_test = le.transform(y_test)\ny_val = le.transform(y_val)\n\ny_train = to_categorical(y_train)\ny_test = to_categorical(y_test)\ny_val = to_categorical(y_val)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T15:37:17.189972Z","iopub.execute_input":"2022-08-21T15:37:17.190576Z","iopub.status.idle":"2022-08-21T15:37:17.199489Z","shell.execute_reply.started":"2022-08-21T15:37:17.190534Z","shell.execute_reply":"2022-08-21T15:37:17.198669Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"\n# Tokenize words\ntokenizer = Tokenizer()\ntokenizer.fit_on_texts(pd.concat([X_train, X_test], axis=0))\n\n\nsequences_train = tokenizer.texts_to_sequences(X_train)\nsequences_test = tokenizer.texts_to_sequences(X_test)\nsequences_val = tokenizer.texts_to_sequences(X_val)\n\nX_train = pad_sequences(sequences_train, maxlen=256, truncating='pre')\nX_test = pad_sequences(sequences_test, maxlen=256, truncating='pre')\nX_val = pad_sequences(sequences_val, maxlen=256, truncating='pre')\n\nvocabSize = len(tokenizer.index_word) + 1\nprint(f\"Vocabulary size = {vocabSize}\")","metadata":{"execution":{"iopub.status.busy":"2022-08-21T15:37:20.987684Z","iopub.execute_input":"2022-08-21T15:37:20.988258Z","iopub.status.idle":"2022-08-21T15:37:21.679345Z","shell.execute_reply.started":"2022-08-21T15:37:20.988219Z","shell.execute_reply":"2022-08-21T15:37:21.678554Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"# Read GloVE embeddings\n\npath_to_glove_file = '../input/glove-global-vectors-for-word-representation/glove.6B.200d.txt'\nnum_tokens = vocabSize\nembedding_dim = 200\nhits = 0\nmisses = 0\nembeddings_index = {}\n\n# Read word vectors\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index[word] = coefs\nprint(\"Found %s word vectors.\" % len(embeddings_index))\n\n\n# Assign word vectors to our dictionary/vocabulary\nembedding_matrix = np.zeros((num_tokens, embedding_dim))\nfor word, i in tokenizer.word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # Words not found in embedding index will be all-zeros.\n        # This includes the representation for \"padding\" and \"OOV\"\n        embedding_matrix[i] = embedding_vector\n        hits += 1\n    else:\n        misses += 1\nprint(\"Converted %d words (%d misses)\" % (hits, misses))","metadata":{"execution":{"iopub.status.busy":"2022-08-21T15:37:31.296977Z","iopub.execute_input":"2022-08-21T15:37:31.297918Z","iopub.status.idle":"2022-08-21T15:37:45.576366Z","shell.execute_reply.started":"2022-08-21T15:37:31.297839Z","shell.execute_reply":"2022-08-21T15:37:45.575561Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Build neural network architecture\n\nadam = Adam(learning_rate=0.005)\n\nmodel = Sequential()\nmodel.add(Embedding(vocabSize, 200, input_length=X_train.shape[1], weights=[embedding_matrix], trainable=False))\nmodel.add(Bidirectional(LSTM(256, dropout=0.2,recurrent_dropout=0.2, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(128, dropout=0.2,recurrent_dropout=0.2, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(128, dropout=0.2,recurrent_dropout=0.2)))\nmodel.add(Dense(4, activation='softmax'))\n\nmodel.compile(loss='categorical_crossentropy', optimizer=adam, metrics=['accuracy'])\nmodel.summary()","metadata":{"execution":{"iopub.status.busy":"2022-08-21T15:38:19.266353Z","iopub.execute_input":"2022-08-21T15:38:19.267103Z","iopub.status.idle":"2022-08-21T15:38:19.944762Z","shell.execute_reply.started":"2022-08-21T15:38:19.267064Z","shell.execute_reply":"2022-08-21T15:38:19.944007Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"callback = EarlyStopping(\n    monitor=\"val_loss\",\n    patience=2,\n    restore_best_weights=True,\n)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T15:38:26.300913Z","iopub.execute_input":"2022-08-21T15:38:26.301618Z","iopub.status.idle":"2022-08-21T15:38:26.305323Z","shell.execute_reply.started":"2022-08-21T15:38:26.301583Z","shell.execute_reply":"2022-08-21T15:38:26.304314Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"# Fit model\nhistory = model.fit(X_train,\n                    y_train,\n                    validation_data=(X_test, y_test),\n                    verbose=1,\n                    batch_size=256,\n                    epochs=3,\n                    callbacks=[callback]\n                   )","metadata":{"execution":{"iopub.status.busy":"2022-08-21T15:44:52.940342Z","iopub.execute_input":"2022-08-21T15:44:52.940983Z","iopub.status.idle":"2022-08-21T16:01:44.268603Z","shell.execute_reply.started":"2022-08-21T15:44:52.940942Z","shell.execute_reply":"2022-08-21T16:01:44.267819Z"},"trusted":true},"execution_count":19,"outputs":[]},{"cell_type":"code","source":"model.evaluate(X_val, y_val, verbose=1)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T16:09:18.974538Z","iopub.execute_input":"2022-08-21T16:09:18.974938Z","iopub.status.idle":"2022-08-21T16:09:39.488331Z","shell.execute_reply.started":"2022-08-21T16:09:18.974891Z","shell.execute_reply":"2022-08-21T16:09:39.487650Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"predict_y = []\ntrue_y = df_test.Emotion.values.tolist()\nfor sentence in df_test.Text:\n    sentence = clean(sentence)\n    sentence = tokenizer.texts_to_sequences([sentence])\n    sentence = pad_sequences(sentence, maxlen=256, truncating='pre')\n    result = le.inverse_transform(np.argmax(model.predict(sentence), axis=-1))[0]\n    predict_y.append(result)","metadata":{"execution":{"iopub.status.busy":"2022-08-21T16:20:02.260160Z","iopub.execute_input":"2022-08-21T16:20:02.260686Z","iopub.status.idle":"2022-08-21T16:31:23.096698Z","shell.execute_reply.started":"2022-08-21T16:20:02.260648Z","shell.execute_reply":"2022-08-21T16:31:23.095913Z"},"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from sklearn import metrics\nprint(metrics.classification_report(true_y, predict_y, digits=4))","metadata":{"execution":{"iopub.status.busy":"2022-08-21T16:31:23.098498Z","iopub.execute_input":"2022-08-21T16:31:23.098744Z","iopub.status.idle":"2022-08-21T16:31:23.120551Z","shell.execute_reply.started":"2022-08-21T16:31:23.098710Z","shell.execute_reply":"2022-08-21T16:31:23.119719Z"},"trusted":true},"execution_count":35,"outputs":[]},{"cell_type":"code","source":"# Visualize Loss & Accuracy\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nacc = history.history['accuracy']\nval_acc = history.history['val_accuracy']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'r', label='Training accuracy')\nplt.plot(epochs, val_acc, 'b', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'r', label='Training Loss')\nplt.plot(epochs, val_loss, 'b', label='Validation Loss')\nplt.title('Training and validation loss')\nplt.legend()\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2022-08-21T15:44:45.657174Z","iopub.status.idle":"2022-08-21T15:44:45.657777Z","shell.execute_reply.started":"2022-08-21T15:44:45.657550Z","shell.execute_reply":"2022-08-21T15:44:45.657573Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Classify custom sample\n\nsentences = [\n            \"He's over the moon about being accepted to the university\",\n            \"Your point on this certain matter made me outrageous, how can you say so? This is insane.\",\n            \"I can't do it, I'm not ready to lose anything, just leave me alone\",\n            \"Merlin's beard harry, you can cast the Patronus charm! I'm amazed!\"\n            ]\nfor sentence in sentences:\n    print(sentence)\n    sentence = clean(sentence)\n    sentence = tokenizer.texts_to_sequences([sentence])\n    sentence = pad_sequences(sentence, maxlen=256, truncating='pre')\n    result = le.inverse_transform(np.argmax(model.predict(sentence), axis=-1))[0]\n    proba =  np.max(model.predict(sentence))\n    print(f\"{result} : {proba}\\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2022-08-21T16:11:10.661312Z","iopub.execute_input":"2022-08-21T16:11:10.661972Z","iopub.status.idle":"2022-08-21T16:11:14.480001Z","shell.execute_reply.started":"2022-08-21T16:11:10.661933Z","shell.execute_reply":"2022-08-21T16:11:14.479210Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import pickle\nwith open('tokenizer.pickle', 'wb') as f:\n    pickle.dump(tokenizer, f)\n    \nwith open('labelEncoder.pickle', 'wb') as f:\n    pickle.dump(le, f)\n    \n    \nmodel.save('Emotion Recognition.h5')","metadata":{"execution":{"iopub.status.busy":"2022-08-21T16:37:28.517888Z","iopub.execute_input":"2022-08-21T16:37:28.518182Z","iopub.status.idle":"2022-08-21T16:37:28.655227Z","shell.execute_reply.started":"2022-08-21T16:37:28.518152Z","shell.execute_reply":"2022-08-21T16:37:28.654253Z"},"trusted":true},"execution_count":37,"outputs":[]}]}